{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Regression mean continous .So a Linear regression model predicts the  values on any continous point in coming from the same distribtion of data from where the trainig data comes. For example Predictng the crop yield in a year , predicting the growth of a company , predicting the rainfall, predicting stock market prices , etc .\n",
    "\n",
    "The model is trained with previous data and generates output for new data. The important thing to note that both of the training and testing data must come from the same distribtion. If we train and test from different distribtion of data our model will behve very bad.\n",
    "Key note is we have no access to that distribtion that why actually behind the seans we are estimating to close enough to that distribtion of data. (refer Cornell university machine learning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Predicing\n",
    "\n",
    "In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a biasÂ :\n",
    "\n",
    "y_pred = w * x_inp + b     \n",
    "\n",
    "our model is repr. by above equations where y is value predicted by our model for a given features x (features can be multiple ) \n",
    "\n",
    "??we need to choose value for w and b for this model . How we do that?\n",
    "\n",
    "### Terriable idea 1\n",
    "\n",
    "Choose any value for w and b then train our model.\n",
    "if we go for this apporch then our model will behave very bad on real world . (Check yourself)\n",
    "\n",
    "\n",
    "Model predicted bad means we have a loss (its data so it can be repr via a no)\n",
    "\n",
    "# Perfect approch\n",
    "\n",
    "## Loss function\n",
    "\n",
    "There are many loss functon present in the real world like MSE, L2,L1 etc. \n",
    "so we choose a loss function to calculate loss and based on this loss we  will adjust the w and b.But How???\n",
    "\n",
    "As we know that in lineat regression we are making a plane or line in between our data point . So for high level intuation (this is not true becoz there are many w and one single b) we can say that this line we need a better gradint(also called slope) and intercept a  represent by w and b respectively . So to calculate w we have something called Gradient Descent Algorithm\n",
    "\n",
    "## Gradient Descent Algorithm \n",
    "\n",
    "Gradient descent is a optimizing algo by which we are adjusting (optimizing) the value of w and b(we can also get rid of w by multiplying w vector with a vector having 1 and x vector with b we will see that later) we calcualte gradient for both and use :\n",
    "\n",
    "\n",
    "The loss is a [quadratic function](https://en.wikipedia.org/wiki/Quadratic_function) of our weights and biases, and our objective is to find the set of weights where the loss is the lowest. If we plot a graph of the loss w.r.t any individual weight or bias element, it will look like the figure shown below. A key insight from calculus is that the gradient indicates the rate of change of the loss, or the [slope](https://en.wikipedia.org/wiki/Slope) of the loss function w.r.t. the weights and biases.\n",
    "\n",
    "If a gradient element is **positive**:\n",
    "* **increasing** the element's value slightly will **increase** the loss.\n",
    "* **decreasing** the element's value slightly will **decrease** the loss\n",
    "\n",
    "![postive-gradient](https://i.imgur.com/hFYoVgU.png)\n",
    "\n",
    "If a gradient element is **negative**:\n",
    "* **increasing** the element's value slightly will **decrease** the loss.\n",
    "* **decreasing** the element's value slightly will **increase** the loss.\n",
    "\n",
    "![negative=gradient](https://i.imgur.com/w3Wii7C.png)\n",
    "\n",
    "The increase or decrease in loss by changing a weight element is proportional to the value of the gradient of the loss w.r.t. that element. This forms the basis for the optimization algorithm that we'll use to improve our model.\n",
    "\n",
    "\n",
    "Optimization is perforned by:\n",
    "\n",
    " w -= w.grad * 1e-5\n",
    " \n",
    " b -= b.grad * 1e-5\n",
    "\n",
    "\n",
    "#### Note:- \n",
    "While caculating gradient we need to set learning rate . and it matters alot  \n",
    "\n",
    "\n",
    "finally we reach a state where we have least loss so we choose this model for real world prediction.\n",
    "\n",
    "### notes to remember \n",
    "1   In pytorch we need to set grad to 0 after calculating grad because remembers prev calculated grad\n",
    "2   In sikit learn and tensorflow learning rate can be setteled automatically\n",
    "\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "We'll reduce the loss and improve our model using the gradient descent optimization algorithm, which has the following steps:\n",
    "\n",
    "1. Generate predictions\n",
    "\n",
    "2. Calculate the loss\n",
    "\n",
    "3. Compute gradients w.r.t the weights and biases\n",
    "\n",
    "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "\n",
    "5. Reset the gradients to zero\n",
    "\n",
    "\n",
    "\n",
    "Initially if we direct put values in the abve eqs then our model will predict values badley \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
